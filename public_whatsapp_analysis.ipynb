{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809e97ac",
   "metadata": {},
   "source": [
    "# WhatsApp Chat Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of WhatsApp chat data including:\n",
    "\n",
    "## Analysis Features:\n",
    "1. **Basic Frequency Analysis**: Message counts, user activity patterns\n",
    "2. **Temporal Analysis**: Time-based messaging patterns, peak days\n",
    "3. **Emoji & Word Analysis**: Most used emojis, word frequency, longest messages\n",
    "4. **Video Call Analysis**: Duration tracking and patterns\n",
    "5. **Sentiment Analysis**: Positive/negative message classification and trends\n",
    "6. **Advanced Pattern Analysis**: Specific word usage tracking over time\n",
    "\n",
    "**Data Source**: Parsed WhatsApp chat data from `whatsapp_parsed_data.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c90eb6",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Import necessary libraries and load the parsed WhatsApp chat data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e06a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import emoji\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b117b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parsed WhatsApp data\n",
    "df = pd.read_csv('data/whatsapp_parsed_data.csv')\n",
    "\n",
    "# Create proper datetime column\n",
    "df['date_str'] = df['Day'].astype(str) + '/' + df['Month'].astype(str) + '/' + df['Year'].astype(str)\n",
    "df['datetime'] = pd.to_datetime(df['date_str'] + ' ' + df['Time'], format='%d/%m/%Y %I:%M:%S %p')\n",
    "df['date'] = df['datetime'].dt.date\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['weekday'] = df['datetime'].dt.day_name()\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total messages: {len(df):,}\")\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "print(f\"Users: {df['User'].unique()}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3c3fa",
   "metadata": {},
   "source": [
    "## 2. Basic Text Frequency Analysis\n",
    "\n",
    "Analyze message frequency patterns and user activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31789b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages sent per user\n",
    "user_message_counts = df['User'].value_counts()\n",
    "print(\"Messages per user:\")\n",
    "for user, count in user_message_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{user}: {count:,} messages ({percentage:.1f}%)\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart\n",
    "user_message_counts.plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title('Messages Sent Per User')\n",
    "axes[0].set_xlabel('User')\n",
    "axes[0].set_ylabel('Number of Messages')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(user_message_counts.values, labels=user_message_counts.index, autopct='%1.1f%%', \n",
    "           colors=['#FF6B6B', '#4ECDC4'], startangle=90)\n",
    "axes[1].set_title('Message Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive plotly chart\n",
    "fig_plotly = px.bar(x=user_message_counts.index, y=user_message_counts.values,\n",
    "                   title=\"Messages Sent Per User (Interactive)\",\n",
    "                   labels={'x': 'User', 'y': 'Number of Messages'},\n",
    "                   color=user_message_counts.index)\n",
    "fig_plotly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e23d8",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis\n",
    "\n",
    "Analyze messaging patterns over time including monthly, yearly, and daily trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages by month and year\n",
    "df['year_month'] = df['datetime'].dt.to_period('M')\n",
    "monthly_counts = df.groupby('year_month').size()\n",
    "yearly_counts = df.groupby('Year').size()\n",
    "\n",
    "# Plotting monthly trends\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Monthly trend\n",
    "monthly_counts.plot(kind='line', ax=axes[0], marker='o', linewidth=2, markersize=4)\n",
    "axes[0].set_title('Messages Per Month Over Time')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Number of Messages')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Yearly trend\n",
    "yearly_counts.plot(kind='bar', ax=axes[1], color='skyblue', alpha=0.8)\n",
    "axes[1].set_title('Messages Per Year')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Number of Messages')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top months and years\n",
    "print(\"Top 5 most active months:\")\n",
    "print(monthly_counts.nlargest(5))\n",
    "print(\"\\nMessages by year:\")\n",
    "print(yearly_counts)\n",
    "\n",
    "# Interactive time series plot\n",
    "fig_interactive = px.line(x=monthly_counts.index.astype(str), y=monthly_counts.values,\n",
    "                         title=\"Monthly Message Frequency (Interactive)\",\n",
    "                         labels={'x': 'Month', 'y': 'Number of Messages'})\n",
    "fig_interactive.update_traces(mode='lines+markers')\n",
    "fig_interactive.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78820ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily analysis - find peak messaging days\n",
    "daily_counts = df.groupby('date').size().sort_values(ascending=False)\n",
    "daily_counts_by_user = df.groupby(['date', 'User']).size().unstack(fill_value=0)\n",
    "\n",
    "print(\"Top 10 most active days (total messages):\")\n",
    "for i, (date, count) in enumerate(daily_counts.head(10).items(), 1):\n",
    "    print(f\"{i}. {date}: {count} messages\")\n",
    "\n",
    "print(\"\\nPeak messaging days by user:\")\n",
    "for user in df['User'].unique():\n",
    "    user_daily = df[df['User'] == user].groupby('date').size()\n",
    "    peak_day = user_daily.idxmax()\n",
    "    peak_count = user_daily.max()\n",
    "    print(f\"{user}: {peak_day} ({peak_count} messages)\")\n",
    "\n",
    "# Weekday analysis\n",
    "weekday_counts = df.groupby('weekday').size()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_counts = weekday_counts.reindex(weekday_order)\n",
    "\n",
    "# Hour analysis\n",
    "hourly_counts = df.groupby('hour').size()\n",
    "\n",
    "# Create subplots for daily patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Top 30 days bar chart\n",
    "daily_counts.head(30).plot(kind='bar', ax=axes[0,0], color='coral')\n",
    "axes[0,0].set_title('Top 30 Most Active Days')\n",
    "axes[0,0].set_xlabel('Date')\n",
    "axes[0,0].set_ylabel('Messages')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Weekday distribution\n",
    "weekday_counts.plot(kind='bar', ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Messages by Day of Week')\n",
    "axes[0,1].set_xlabel('Day of Week')\n",
    "axes[0,1].set_ylabel('Messages')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hourly distribution\n",
    "hourly_counts.plot(kind='line', ax=axes[1,0], marker='o', color='purple')\n",
    "axes[1,0].set_title('Messages by Hour of Day')\n",
    "axes[1,0].set_xlabel('Hour')\n",
    "axes[1,0].set_ylabel('Messages')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap of weekday vs hour\n",
    "pivot_data = df.groupby(['weekday', 'hour']).size().unstack(fill_value=0)\n",
    "pivot_data = pivot_data.reindex(weekday_order)\n",
    "sns.heatmap(pivot_data, ax=axes[1,1], cmap='YlOrRd', cbar_kws={'label': 'Messages'})\n",
    "axes[1,1].set_title('Activity Heatmap: Weekday vs Hour')\n",
    "axes[1,1].set_xlabel('Hour')\n",
    "axes[1,1].set_ylabel('Day of Week')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b2f96",
   "metadata": {},
   "source": [
    "## 4. Emoji and Word Analysis\n",
    "\n",
    "Analyze emoji usage patterns and most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27378d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji analysis\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text\"\"\"\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "# Extract all emojis\n",
    "all_emojis = []\n",
    "user_emojis = {user: [] for user in df['User'].unique()}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    emojis_in_text = extract_emojis(str(row['text_message']))\n",
    "    all_emojis.extend(emojis_in_text)\n",
    "    user_emojis[row['User']].extend(emojis_in_text)\n",
    "\n",
    "# Count emojis\n",
    "emoji_counts = Counter(all_emojis)\n",
    "user_emoji_counts = {user: Counter(emojis) for user, emojis in user_emojis.items()}\n",
    "\n",
    "# Display top emojis\n",
    "print(\"Top 20 Most Used Emojis (Overall):\")\n",
    "for i, (emoji_char, count) in enumerate(emoji_counts.most_common(20), 1):\n",
    "    print(f\"{i:2d}. {emoji_char} : {count:,} times\")\n",
    "\n",
    "print(\"\\nTop 10 Emojis by User:\")\n",
    "for user in df['User'].unique():\n",
    "    print(f\"\\n{user}:\")\n",
    "    for i, (emoji_char, count) in enumerate(user_emoji_counts[user].most_common(10), 1):\n",
    "        print(f\"  {i:2d}. {emoji_char} : {count:,} times\")\n",
    "\n",
    "# Visualize top emojis\n",
    "if emoji_counts:\n",
    "    top_emojis = dict(emoji_counts.most_common(15))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Overall emoji usage\n",
    "    axes[0].bar(range(len(top_emojis)), list(top_emojis.values()), color='gold')\n",
    "    axes[0].set_title('Top 15 Most Used Emojis (Overall)')\n",
    "    axes[0].set_xlabel('Emoji Rank')\n",
    "    axes[0].set_ylabel('Usage Count')\n",
    "    axes[0].set_xticks(range(len(top_emojis)))\n",
    "    axes[0].set_xticklabels(list(top_emojis.keys()), fontsize=16)\n",
    "    \n",
    "    # Emoji usage by user\n",
    "    users = list(df['User'].unique())\n",
    "    top_5_emojis = list(emoji_counts.most_common(5))\n",
    "    \n",
    "    x = np.arange(len(top_5_emojis))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, user in enumerate(users):\n",
    "        user_counts = [user_emoji_counts[user].get(emoji_char, 0) for emoji_char, _ in top_5_emojis]\n",
    "        axes[1].bar(x + i * width, user_counts, width, label=user, alpha=0.8)\n",
    "    \n",
    "    axes[1].set_title('Top 5 Emojis by User')\n",
    "    axes[1].set_xlabel('Emoji')\n",
    "    axes[1].set_ylabel('Usage Count')\n",
    "    axes[1].set_xticks(x + width/2)\n",
    "    axes[1].set_xticklabels([emoji_char for emoji_char, _ in top_5_emojis], fontsize=16)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No emojis found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ae213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_text_for_words(text):\n",
    "    \"\"\"Clean text and extract words\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = str(text).lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Split into words and filter out common stopwords, short words, and media-related terms\n",
    "    stopwords = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', \n",
    "                'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', \n",
    "                'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', \n",
    "                'must', 'can', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', \n",
    "                'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
    "                'this', 'that', 'these', 'those', 'am', 'if', 'then', 'than', 'so', 'very',\n",
    "                'just', 'now', 'get', 'got', 'go', 'went', 'like', 'also', 'well', 'know',\n",
    "                'think', 'want', 'see', 'come', 'came', 'back', 'time', 'good', 'make',\n",
    "                'way', 'even', 'new', 'take', 'say', 'said', 'one', 'two', 'first', 'last',\n",
    "                # Media-related terms to exclude from word analysis\n",
    "                'image', 'omitted', 'video', 'audio', 'document', 'sticker', 'gif'}\n",
    "    \n",
    "    words = [word for word in text.split() if len(word) > 2 and word not in stopwords]\n",
    "    return words\n",
    "\n",
    "# Extract words for all users and individually\n",
    "all_words = []\n",
    "user_words = defaultdict(list)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    words = clean_text_for_words(row['text_message'])\n",
    "    all_words.extend(words)\n",
    "    user_words[row['User']].extend(words)\n",
    "\n",
    "# Count words\n",
    "word_counts = Counter(all_words)\n",
    "user_word_counts = {user: Counter(words) for user, words in user_words.items()}\n",
    "\n",
    "# Display top words\n",
    "print(\"Top 50 Most Common Words (Overall):\")\n",
    "for i, (word, count) in enumerate(word_counts.most_common(50), 1):\n",
    "    print(f\"{i:2d}. {word:15} : {count:,} times\")\n",
    "\n",
    "print(\"\\nTop 20 Words by User:\")\n",
    "for user in df['User'].unique():\n",
    "    print(f\"\\n{user}:\")\n",
    "    for i, (word, count) in enumerate(user_word_counts[user].most_common(20), 1):\n",
    "        print(f\"  {i:2d}. {word:15} : {count:,} times\")\n",
    "\n",
    "# Visualize top words\n",
    "if word_counts:\n",
    "    top_words = dict(word_counts.most_common(20))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Overall word usage\n",
    "    axes[0].barh(range(len(top_words)), list(top_words.values()), color='lightcoral')\n",
    "    axes[0].set_title('Top 20 Most Used Words (Overall)')\n",
    "    axes[0].set_xlabel('Usage Count')\n",
    "    axes[0].set_ylabel('Words')\n",
    "    axes[0].set_yticks(range(len(top_words)))\n",
    "    axes[0].set_yticklabels(list(top_words.keys()))\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Word usage comparison between users\n",
    "    users = list(df['User'].unique())\n",
    "    top_10_words = list(word_counts.most_common(10))\n",
    "    \n",
    "    x = np.arange(len(top_10_words))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, user in enumerate(users):\n",
    "        user_counts = [user_word_counts[user].get(word, 0) for word, _ in top_10_words]\n",
    "        axes[1].bar(x + i * width, user_counts, width, label=user, alpha=0.8)\n",
    "    \n",
    "    axes[1].set_title('Top 10 Words by User Comparison')\n",
    "    axes[1].set_xlabel('Words')\n",
    "    axes[1].set_ylabel('Usage Count')\n",
    "    axes[1].set_xticks(x + width/2)\n",
    "    axes[1].set_xticklabels([word for word, _ in top_10_words], rotation=45)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== WORD CLOUD VISUALIZATION ===\")\n",
    "\n",
    "# Create word clouds for overall and individual users\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Overall word cloud\n",
    "if word_counts:\n",
    "    # Create word frequency dictionary for wordcloud\n",
    "    wordcloud_overall = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        relative_scaling=0.5,\n",
    "        random_state=42\n",
    "    ).generate_from_frequencies(word_counts)\n",
    "    \n",
    "    axes[0,0].imshow(wordcloud_overall, interpolation='bilinear')\n",
    "    axes[0,0].set_title('Overall Word Cloud (All Messages)', fontsize=16, fontweight='bold')\n",
    "    axes[0,0].axis('off')\n",
    "else:\n",
    "    axes[0,0].text(0.5, 0.5, 'No words found', ha='center', va='center', transform=axes[0,0].transAxes)\n",
    "    axes[0,0].set_title('Overall Word Cloud')\n",
    "\n",
    "# Individual user word clouds\n",
    "users = list(df['User'].unique())\n",
    "user_positions = [(0,1), (1,0), (1,1)]  # positions for up to 3 users\n",
    "\n",
    "for i, user in enumerate(users):\n",
    "    if i < len(user_positions):\n",
    "        row, col = user_positions[i]\n",
    "        \n",
    "        if user_word_counts[user]:\n",
    "            # Create user-specific word cloud\n",
    "            user_wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                background_color='white',\n",
    "                colormap='plasma' if i == 0 else 'cool',\n",
    "                max_words=100,\n",
    "                relative_scaling=0.5,\n",
    "                random_state=42\n",
    "            ).generate_from_frequencies(user_word_counts[user])\n",
    "            \n",
    "            axes[row,col].imshow(user_wordcloud, interpolation='bilinear')\n",
    "            axes[row,col].set_title(f'{user} Word Cloud', fontsize=16, fontweight='bold')\n",
    "            axes[row,col].axis('off')\n",
    "        else:\n",
    "            axes[row,col].text(0.5, 0.5, f'No words found for {user}', \n",
    "                              ha='center', va='center', transform=axes[row,col].transAxes)\n",
    "            axes[row,col].set_title(f'{user} Word Cloud')\n",
    "\n",
    "# Hide unused subplot if only 2 users\n",
    "if len(users) == 2:\n",
    "    axes[1,1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create combined word cloud with top words from each user\n",
    "print(\"\\n=== COMBINED USER COMPARISON WORD CLOUD ===\")\n",
    "\n",
    "if len(users) >= 2:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    for i, user in enumerate(users[:2]):  # Show first 2 users\n",
    "        if user_word_counts[user]:\n",
    "            # Get top 50 words for this user\n",
    "            top_user_words = dict(user_word_counts[user].most_common(50))\n",
    "            \n",
    "            user_wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                background_color='white',\n",
    "                colormap='Set1' if i == 0 else 'Set2',\n",
    "                max_words=50,\n",
    "                relative_scaling=0.5,\n",
    "                random_state=42\n",
    "            ).generate_from_frequencies(top_user_words)\n",
    "            \n",
    "            axes[i].imshow(user_wordcloud, interpolation='bilinear')\n",
    "            axes[i].set_title(f'{user} - Top 50 Words', fontsize=16, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7671c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show word cloud statistics\n",
    "print(f\"\\n=== WORD CLOUD STATISTICS ===\")\n",
    "print(f\"Total unique words: {len(word_counts):,}\")\n",
    "print(f\"Most frequent word: '{word_counts.most_common(1)[0][0]}' ({word_counts.most_common(1)[0][1]:,} times)\")\n",
    "\n",
    "for user in users:\n",
    "    user_unique_words = len(user_word_counts[user])\n",
    "    if user_unique_words > 0:\n",
    "        most_frequent_user_word = user_word_counts[user].most_common(1)[0]\n",
    "        print(f\"{user} unique words: {user_unique_words:,}, most frequent: '{most_frequent_user_word[0]}' ({most_frequent_user_word[1]:,} times)\")\n",
    "\n",
    "# Word diversity analysis\n",
    "print(f\"\\n=== WORD DIVERSITY ANALYSIS ===\")\n",
    "total_words = sum(word_counts.values())\n",
    "unique_words = len(word_counts)\n",
    "word_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "\n",
    "print(f\"Total words used: {total_words:,}\")\n",
    "print(f\"Unique words: {unique_words:,}\")\n",
    "print(f\"Word diversity ratio: {word_diversity:.4f} (higher = more diverse vocabulary)\")\n",
    "\n",
    "for user in users:\n",
    "    user_total_words = sum(user_word_counts[user].values())\n",
    "    user_unique_words = len(user_word_counts[user])\n",
    "    user_diversity = user_unique_words / user_total_words if user_total_words > 0 else 0\n",
    "    \n",
    "    print(f\"{user} - Total: {user_total_words:,}, Unique: {user_unique_words:,}, Diversity: {user_diversity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29822d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longest messages analysis\n",
    "df['message_length'] = df['text_message'].astype(str).str.len()\n",
    "\n",
    "print(\"=== LONGEST MESSAGES ANALYSIS ===\")\n",
    "print(f\"Average message length: {df['message_length'].mean():.1f} characters\")\n",
    "print(f\"Median message length: {df['message_length'].median():.1f} characters\")\n",
    "print(f\"Maximum message length: {df['message_length'].max():,} characters\")\n",
    "\n",
    "# Longest messages overall\n",
    "print(\"\\nTop 10 Longest Messages (Overall):\")\n",
    "longest_messages = df.nlargest(10, 'message_length')\n",
    "for i, (idx, row) in enumerate(longest_messages.iterrows(), 1):\n",
    "    preview = str(row['text_message'])[:100] + \"...\" if len(str(row['text_message'])) > 100 else str(row['text_message'])\n",
    "    print(f\"{i:2d}. {row['User']} ({row['message_length']:,} chars): {preview}\")\n",
    "    print(f\"    Date: {row['date']}\")\n",
    "    print()\n",
    "\n",
    "# Longest messages by user\n",
    "print(\"Longest Messages by User:\")\n",
    "for user in df['User'].unique():\n",
    "    user_data = df[df['User'] == user]\n",
    "    longest_msg = user_data.loc[user_data['message_length'].idxmax()]\n",
    "    preview = str(longest_msg['text_message'])[:150] + \"...\" if len(str(longest_msg['text_message'])) > 150 else str(longest_msg['text_message'])\n",
    "    print(f\"\\n{user}: {longest_msg['message_length']:,} characters\")\n",
    "    print(f\"Date: {longest_msg['date']}\")\n",
    "    print(f\"Message: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message length distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['message_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Message Length Distribution')\n",
    "plt.xlabel('Message Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "df.boxplot(column='message_length', by='User', ax=plt.gca())\n",
    "plt.title('Message Length by User')\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Message Length (characters)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "user_avg_length = df.groupby('User')['message_length'].mean()\n",
    "user_avg_length.plot(kind='bar', color=['coral', 'lightgreen'])\n",
    "plt.title('Average Message Length by User')\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Average Length (characters)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Communication time calculation\n",
    "total_days = (df['datetime'].max() - df['datetime'].min()).days\n",
    "total_hours = total_days * 24\n",
    "messages_per_day = len(df) / total_days\n",
    "avg_time_per_message = 20  # Assume 30 seconds per message\n",
    "\n",
    "estimated_texting_time_hours = (len(df) * avg_time_per_message) / 3600\n",
    "estimated_texting_time_days = estimated_texting_time_hours / 24\n",
    "\n",
    "print(f\"\\n=== COMMUNICATION TIME ANALYSIS ===\")\n",
    "print(f\"Total conversation period: {total_days:,} days ({total_hours:,} hours)\")\n",
    "print(f\"Total messages: {len(df):,}\")\n",
    "print(f\"Average messages per day: {messages_per_day:.1f}\")\n",
    "print(f\"Estimated time spent texting: {estimated_texting_time_hours:.1f} hours ({estimated_texting_time_days:.1f} days)\")\n",
    "print(f\"Percentage of time spent texting: {(estimated_texting_time_hours/total_hours)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba2835",
   "metadata": {},
   "source": [
    "## 5. Video Call Duration Analysis\n",
    "\n",
    "Extract and analyze video call durations from messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54768700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video call analysis\n",
    "def extract_video_call_duration(text):\n",
    "    \"\"\"Extract video call duration from text like 'Video call, 45 min'\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Pattern to match \"Video call, X min\" or variations\n",
    "    pattern = r'video call[,\\s]+(\\d+)\\s*min'\n",
    "    match = re.search(pattern, str(text).lower())\n",
    "    \n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Extract video call durations\n",
    "df['video_call_duration'] = df['text_message'].apply(extract_video_call_duration)\n",
    "video_calls = df[df['video_call_duration'].notna()].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(video_calls) > 0:\n",
    "    print(\"=== VIDEO CALL ANALYSIS ===\")\n",
    "    print(f\"Total video calls found: {len(video_calls)}\")\n",
    "    print(f\"Total video call time: {video_calls['video_call_duration'].sum():,} minutes\")\n",
    "    print(f\"Total video call time: {video_calls['video_call_duration'].sum()/60:.1f} hours\")\n",
    "    print(f\"Total video call time: {video_calls['video_call_duration'].sum()/(60*24):.1f} days\")\n",
    "    print(f\"Average call duration: {video_calls['video_call_duration'].mean():.1f} minutes\")\n",
    "    print(f\"Longest call: {video_calls['video_call_duration'].max()} minutes\")\n",
    "    print(f\"Shortest call: {video_calls['video_call_duration'].min()} minutes\")\n",
    "    \n",
    "    # Video calls by user\n",
    "    print(\"\\nVideo Calls by User:\")\n",
    "    call_stats_by_user = video_calls.groupby('User')['video_call_duration'].agg(['count', 'sum', 'mean'])\n",
    "    call_stats_by_user.columns = ['Number of Calls', 'Total Minutes', 'Average Duration']\n",
    "    call_stats_by_user['Total Hours'] = call_stats_by_user['Total Minutes'] / 60\n",
    "    print(call_stats_by_user)\n",
    "    \n",
    "    # Video calls over time\n",
    "    video_calls['call_month'] = video_calls['datetime'].dt.to_period('M')\n",
    "    monthly_calls = video_calls.groupby('call_month')['video_call_duration'].agg(['count', 'sum'])\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Call duration distribution\n",
    "    axes[0,0].hist(video_calls['video_call_duration'], bins=20, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0,0].set_title('Video Call Duration Distribution')\n",
    "    axes[0,0].set_xlabel('Duration (minutes)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Calls by user\n",
    "    call_counts = video_calls['User'].value_counts()\n",
    "    call_counts.plot(kind='bar', ax=axes[0,1], color=['orange', 'green'])\n",
    "    axes[0,1].set_title('Number of Video Calls by User')\n",
    "    axes[0,1].set_xlabel('User')\n",
    "    axes[0,1].set_ylabel('Number of Calls')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Total minutes by user\n",
    "    total_minutes = video_calls.groupby('User')['video_call_duration'].sum()\n",
    "    total_minutes.plot(kind='bar', ax=axes[1,0], color=['coral', 'lightgreen'])\n",
    "    axes[1,0].set_title('Total Video Call Minutes by User')\n",
    "    axes[1,0].set_xlabel('User')\n",
    "    axes[1,0].set_ylabel('Total Minutes')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Calls over time\n",
    "    monthly_calls['count'].plot(kind='line', ax=axes[1,1], marker='o', color='purple')\n",
    "    axes[1,1].set_title('Video Calls per Month')\n",
    "    axes[1,1].set_xlabel('Month')\n",
    "    axes[1,1].set_ylabel('Number of Calls')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show some example video call messages\n",
    "    print(\"\\nExample Video Call Messages:\")\n",
    "    for i, (idx, row) in enumerate(video_calls.head(10).iterrows(), 1):\n",
    "        print(f\"{i}. {row['User']} - {row['date']}: {row['text_message']} ({row['video_call_duration']} min)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No video call messages found in the format 'Video call, X min'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10f2cc",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis\n",
    "\n",
    "Analyze the emotional content of messages and identify sentiment trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990de700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return 0, 'neutral'\n",
    "    \n",
    "    try:\n",
    "        blob = TextBlob(str(text))\n",
    "        polarity = blob.sentiment.polarity\n",
    "        \n",
    "        if polarity > 0.1:\n",
    "            sentiment = 'positive'\n",
    "        elif polarity < -0.1:\n",
    "            sentiment = 'negative'\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "            \n",
    "        return polarity, sentiment\n",
    "    except:\n",
    "        return 0, 'neutral'\n",
    "\n",
    "# Apply sentiment analysis\n",
    "print(\"Analyzing sentiment for all messages...\")\n",
    "sentiment_results = df['text_message'].apply(analyze_sentiment)\n",
    "df['sentiment_polarity'] = [result[0] for result in sentiment_results]\n",
    "df['sentiment_label'] = [result[1] for result in sentiment_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd044165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== SENTIMENT ANALYSIS RESULTS ===\")\n",
    "\n",
    "# Overall sentiment distribution\n",
    "sentiment_counts = df['sentiment_label'].value_counts()\n",
    "print(\"Overall Sentiment Distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{sentiment}: {count:,} messages ({percentage:.1f}%)\")\n",
    "\n",
    "# Sentiment by user\n",
    "print(\"\\nSentiment Distribution by User:\")\n",
    "sentiment_by_user = df.groupby(['User', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sentiment_by_user_pct = sentiment_by_user.div(sentiment_by_user.sum(axis=1), axis=0) * 100\n",
    "\n",
    "for user in df['User'].unique():\n",
    "    print(f\"\\n{user}:\")\n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        if sentiment in sentiment_by_user.columns:\n",
    "            count = sentiment_by_user.loc[user, sentiment]\n",
    "            pct = sentiment_by_user_pct.loc[user, sentiment]\n",
    "            print(f\"  {sentiment.capitalize()}: {count:,} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Overall sentiment pie chart\n",
    "colors = ['lightgreen', 'lightgray', 'lightcoral']\n",
    "sentiment_counts.plot(kind='pie', ax=axes[0,0], autopct='%1.1f%%', colors=colors)\n",
    "axes[0,0].set_title('Overall Sentiment Distribution')\n",
    "axes[0,0].set_ylabel('')\n",
    "\n",
    "# Sentiment by user\n",
    "sentiment_by_user.plot(kind='bar', ax=axes[0,1], color=colors)\n",
    "axes[0,1].set_title('Sentiment Count by User')\n",
    "axes[0,1].set_xlabel('User')\n",
    "axes[0,1].set_ylabel('Number of Messages')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Sentiment polarity distribution\n",
    "axes[1,0].hist(df['sentiment_polarity'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,0].set_title('Sentiment Polarity Distribution')\n",
    "axes[1,0].set_xlabel('Polarity Score (-1 to 1)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Average sentiment by user\n",
    "avg_sentiment = df.groupby('User')['sentiment_polarity'].mean()\n",
    "avg_sentiment.plot(kind='bar', ax=axes[1,1], color=['orange', 'green'])\n",
    "axes[1,1].set_title('Average Sentiment Polarity by User')\n",
    "axes[1,1].set_xlabel('User')\n",
    "axes[1,1].set_ylabel('Average Polarity')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee313df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal sentiment analysis\n",
    "df['sentiment_month'] = df['datetime'].dt.to_period('M')\n",
    "\n",
    "# Monthly sentiment trends\n",
    "monthly_sentiment = df.groupby(['sentiment_month', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "monthly_avg_sentiment = df.groupby('sentiment_month')['sentiment_polarity'].mean()\n",
    "\n",
    "# Daily sentiment analysis\n",
    "daily_sentiment = df.groupby('date')['sentiment_polarity'].mean().sort_values()\n",
    "\n",
    "# Find most positive and negative days\n",
    "most_negative_day = daily_sentiment.idxmin()\n",
    "most_positive_day = daily_sentiment.idxmax()\n",
    "\n",
    "print(\"=== TEMPORAL SENTIMENT ANALYSIS ===\")\n",
    "print(f\"Most negative day: {most_negative_day} (avg polarity: {daily_sentiment.min():.3f})\")\n",
    "print(f\"Most positive day: {most_positive_day} (avg polarity: {daily_sentiment.max():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7981184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Messages from most negative day\n",
    "negative_day_messages = df[df['date'] == most_negative_day].sort_values('sentiment_polarity')\n",
    "print(f\"\\nMessages from most negative day ({most_negative_day}):\")\n",
    "print(f\"Total messages: {len(negative_day_messages)}\")\n",
    "print(\"Most negative messages:\")\n",
    "for i, (idx, row) in enumerate(negative_day_messages.head(5).iterrows(), 1):\n",
    "    preview = str(row['text_message'])[:100] + \"...\" if len(str(row['text_message'])) > 100 else str(row['text_message'])\n",
    "    print(f\"{i}. {row['User']} (polarity: {row['sentiment_polarity']:.3f}): {preview}\")\n",
    "\n",
    "# Messages from most positive day\n",
    "positive_day_messages = df[df['date'] == most_positive_day].sort_values('sentiment_polarity', ascending=False)\n",
    "print(f\"\\nMessages from most positive day ({most_positive_day}):\")\n",
    "print(f\"Total messages: {len(positive_day_messages)}\")\n",
    "print(\"Most positive messages:\")\n",
    "for i, (idx, row) in enumerate(positive_day_messages.head(5).iterrows(), 1):\n",
    "    preview = str(row['text_message'])[:100] + \"...\" if len(str(row['text_message'])) > 100 else str(row['text_message'])\n",
    "    print(f\"{i}. {row['User']} (polarity: {row['sentiment_polarity']:.3f}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot temporal sentiment trends\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Monthly sentiment trends\n",
    "monthly_sentiment.plot(kind='area', ax=axes[0], alpha=0.7, color=['lightcoral', 'lightgray', 'lightgreen'])\n",
    "axes[0].set_title('Monthly Sentiment Trends (Message Count)')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Number of Messages')\n",
    "axes[0].legend()\n",
    "\n",
    "# Daily average sentiment (moving average for smoother visualization)\n",
    "daily_sentiment_smooth = daily_sentiment.rolling(window=7).mean()\n",
    "daily_sentiment_smooth.plot(kind='line', ax=axes[1], color='purple', alpha=0.8)\n",
    "axes[1].set_title('Daily Average Sentiment (7-day moving average)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Average Sentiment Polarity')\n",
    "axes[1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight extreme days\n",
    "axes[1].scatter([most_negative_day], [daily_sentiment.min()], color='red', s=100, zorder=5, label='Most Negative Day')\n",
    "axes[1].scatter([most_positive_day], [daily_sentiment.max()], color='green', s=100, zorder=5, label='Most Positive Day')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1797fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sentiment by month and year for each user\n",
    "print(\"\\nMonthly Sentiment by User:\")\n",
    "for user in df['User'].unique():\n",
    "    user_data = df[df['User'] == user]\n",
    "    user_monthly_sentiment = user_data.groupby(['sentiment_month', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "    print(f\"\\n{user} - Monthly positive/negative message ratio:\")\n",
    "    \n",
    "    if 'positive' in user_monthly_sentiment.columns and 'negative' in user_monthly_sentiment.columns:\n",
    "        ratio = user_monthly_sentiment['positive'] / (user_monthly_sentiment['negative'] + 1)  # +1 to avoid division by zero\n",
    "        print(f\"Average positive/negative ratio: {ratio.mean():.2f}\")\n",
    "        print(\"Top 3 most positive months:\")\n",
    "        print(ratio.nlargest(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710765cc",
   "metadata": {},
   "source": [
    "## 7. Specific Word Pattern Analysis\n",
    "\n",
    "This section tracks specific words/patterns mentioned in our requirements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f25948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track specific words/patterns over time\n",
    "import re\n",
    "\n",
    "# Define specific patterns to track\n",
    "patterns = {\n",
    "    'pizza': r'\\bpizza\\b',\n",
    "    'love': r'\\blove\\b',\n",
    "    'sorry': r'\\bsorry\\b',\n",
    "}\n",
    "\n",
    "def find_pattern_matches(text, pattern):\n",
    "    \"\"\"Find all matches of a pattern in text (case insensitive)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return re.findall(pattern, str(text).lower(), re.IGNORECASE)\n",
    "\n",
    "# Create columns for each pattern\n",
    "pattern_data = {}\n",
    "for pattern_name, pattern_regex in patterns.items():\n",
    "    df[f'{pattern_name}_matches'] = df['text_message'].apply(lambda x: find_pattern_matches(x, pattern_regex))\n",
    "    df[f'{pattern_name}_count'] = df[f'{pattern_name}_matches'].apply(len)\n",
    "    pattern_data[pattern_name] = df[f'{pattern_name}_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a848cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SPECIFIC WORD PATTERN ANALYSIS ===\")\n",
    "print(\"Total usage count for each pattern:\")\n",
    "for pattern, count in pattern_data.items():\n",
    "    print(f\"{pattern}: {count} occurrences\")\n",
    "\n",
    "# Temporal analysis of pattern usage\n",
    "monthly_patterns = pd.DataFrame()\n",
    "for pattern_name in patterns.keys():\n",
    "    monthly_count = df.groupby(df['datetime'].dt.to_period('M'))[f'{pattern_name}_count'].sum()\n",
    "    monthly_patterns[pattern_name] = monthly_count\n",
    "\n",
    "# Find peak usage months for each pattern\n",
    "print(\"\\nPeak usage months for each pattern:\")\n",
    "for pattern in patterns.keys():\n",
    "    if monthly_patterns[pattern].sum() > 0:\n",
    "        peak_month = monthly_patterns[pattern].idxmax()\n",
    "        peak_count = monthly_patterns[pattern].max()\n",
    "        print(f\"{pattern}: {peak_month} ({peak_count} occurrences)\")\n",
    "\n",
    "# User analysis for patterns\n",
    "print(\"\\nPattern usage by user:\")\n",
    "for pattern_name in patterns.keys():\n",
    "    user_pattern_usage = df.groupby('User')[f'{pattern_name}_count'].sum().sort_values(ascending=False)\n",
    "    if user_pattern_usage.sum() > 0:\n",
    "        print(f\"\\n{pattern_name}:\")\n",
    "        for user, count in user_pattern_usage.items():\n",
    "            if count > 0:\n",
    "                percentage = (count / user_pattern_usage.sum()) * 100\n",
    "                print(f\"  {user}: {count} times ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea218451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot pattern usage over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select top 4 most used patterns for visualization\n",
    "top_patterns = sorted(pattern_data.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "\n",
    "for i, (pattern_name, total_count) in enumerate(top_patterns):\n",
    "    if total_count > 0:\n",
    "        monthly_usage = monthly_patterns[pattern_name]\n",
    "        monthly_usage.plot(kind='line', ax=axes[i], marker='o', linewidth=2)\n",
    "        axes[i].set_title(f\"'{pattern_name}' usage over time (Total: {total_count})\")\n",
    "        axes[i].set_xlabel('Month')\n",
    "        axes[i].set_ylabel('Usage Count')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(top_patterns), 4):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67979200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a heatmap showing pattern usage by user and month\n",
    "if len([p for p in pattern_data.values() if p > 0]) > 0:\n",
    "    # Create user-pattern matrix\n",
    "    user_pattern_matrix = pd.DataFrame()\n",
    "    for user in df['User'].unique():\n",
    "        user_data = df[df['User'] == user]\n",
    "        user_totals = {}\n",
    "        for pattern_name in patterns.keys():\n",
    "            user_totals[pattern_name] = user_data[f'{pattern_name}_count'].sum()\n",
    "        user_pattern_matrix[user] = pd.Series(user_totals)\n",
    "    \n",
    "    # Only show patterns/users with actual usage\n",
    "    user_pattern_matrix = user_pattern_matrix.loc[(user_pattern_matrix > 0).any(axis=1), \n",
    "                                                  (user_pattern_matrix > 0).any(axis=0)]\n",
    "    \n",
    "    if not user_pattern_matrix.empty:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(user_pattern_matrix, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Usage Count'})\n",
    "        plt.title('Pattern Usage by User (Heatmap)')\n",
    "        plt.xlabel('Users')\n",
    "        plt.ylabel('Patterns')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a77ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample messages for interesting patterns\n",
    "interesting_patterns = ['pizza', 'love', 'sorry']\n",
    "for pattern in interesting_patterns:\n",
    "    pattern_messages = df[df[f'{pattern}_count'] > 0]\n",
    "    if len(pattern_messages) > 0:\n",
    "        print(f\"\\nSample messages containing '{pattern}':\")\n",
    "        for i, (idx, row) in enumerate(pattern_messages.sample(min(3, len(pattern_messages))).iterrows(), 1):\n",
    "            preview = str(row['text_message'])[:150] + \"...\" if len(str(row['text_message'])) > 150 else str(row['text_message'])\n",
    "            print(f\"{i}. {row['User']} ({row['date']}): {preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ef222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First instances and chronological context analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIRST INSTANCES AND CHRONOLOGICAL CONTEXT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Target patterns for detailed analysis\n",
    "target_patterns = list(patterns.keys())\n",
    "\n",
    "for pattern in target_patterns:\n",
    "    pattern_messages = df[df[f'{pattern}_count'] > 0].copy()\n",
    "    \n",
    "    if len(pattern_messages) > 0:\n",
    "        # Sort by datetime to find the first instance\n",
    "        pattern_messages_sorted = pattern_messages.sort_values('datetime')\n",
    "        first_instance = pattern_messages_sorted.iloc[0]\n",
    "        \n",
    "        print(f\"\\nüîç FIRST INSTANCE OF '{pattern.upper()}':\")\n",
    "        print(f\"Date: {first_instance['date']}\")\n",
    "        print(f\"Time: {first_instance['Time']}\")\n",
    "        print(f\"User: {first_instance['User']}\")\n",
    "        print(f\"Message: {first_instance['text_message']}\")\n",
    "        \n",
    "        # Find the index of this message in the original dataframe\n",
    "        first_msg_index = first_instance.name\n",
    "        \n",
    "        # Get the next 5 messages chronologically (by datetime, not by index)\n",
    "        first_msg_datetime = first_instance['datetime']\n",
    "        \n",
    "        # Find all messages after this datetime\n",
    "        subsequent_messages = df[df['datetime'] > first_msg_datetime].sort_values('datetime').head(5)\n",
    "        \n",
    "        if len(subsequent_messages) > 0:\n",
    "            print(f\"\\nüìù NEXT 5 MESSAGES AFTER FIRST '{pattern.upper()}' INSTANCE:\")\n",
    "            for i, (idx, row) in enumerate(subsequent_messages.iterrows(), 1):\n",
    "                time_diff = (row['datetime'] - first_msg_datetime).total_seconds()\n",
    "                hours = int(time_diff // 3600)\n",
    "                minutes = int((time_diff % 3600) // 60)\n",
    "                \n",
    "                if hours > 0:\n",
    "                    time_str = f\"({hours}h {minutes}m later)\"\n",
    "                else:\n",
    "                    time_str = f\"({minutes}m later)\"\n",
    "                \n",
    "                print(f\"{i}. {row['User']} - {row['date']} {row['Time']} {time_str}\")\n",
    "                print(f\"   {row['text_message']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"No subsequent messages found after the first '{pattern}' instance.\")\n",
    "        \n",
    "        # Additional context: Show total occurrences and date range\n",
    "        total_occurrences = pattern_messages[f'{pattern}_count'].sum()\n",
    "        first_date = pattern_messages_sorted.iloc[0]['date']\n",
    "        last_date = pattern_messages_sorted.iloc[-1]['date']\n",
    "        \n",
    "        print(f\"üìä Pattern Summary:\")\n",
    "        print(f\"   Total occurrences: {total_occurrences}\")\n",
    "        print(f\"   First used: {first_date}\")\n",
    "        print(f\"   Last used: {last_date}\")\n",
    "        print(f\"   Usage span: {(pd.to_datetime(last_date) - pd.to_datetime(first_date)).days} days\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå No instances of '{pattern.upper()}' found in the conversation.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d644c77",
   "metadata": {},
   "source": [
    "## 8. Good Morning/Good Night Analysis\n",
    "\n",
    "This section analyzes the frequency of \"good morning\" and \"good night\" messages by user and over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Morning and Good Night Analysis\n",
    "import re\n",
    "\n",
    "def detect_good_morning(text):\n",
    "    \"\"\"Detect good morning messages (various patterns)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Patterns for good morning\n",
    "    gm_patterns = [\n",
    "        r'\\bgood\\s*morning\\b',\n",
    "        r'\\bgd\\s*morning\\b',\n",
    "        r'\\bg\\s*morning\\b',\n",
    "        r'\\bmorning\\b(?!\\s*(shift|workout|run|jog))',  # morning but not morning shift/workout\n",
    "        r'\\bgm\\b(?!\\s*(shift|car|music))',  # gm but not GM shift/car/music\n",
    "        r'\\bgood\\s*mrng\\b',\n",
    "        r'\\bgutten\\s*morgen\\b',  # German\n",
    "        r'\\bbuenos\\s*dias\\b',    # Spanish\n",
    "        r'\\bbonjour\\b'           # French\n",
    "    ]\n",
    "    \n",
    "    return any(re.search(pattern, text) for pattern in gm_patterns)\n",
    "\n",
    "def detect_good_night(text):\n",
    "    \"\"\"Detect good night messages (various patterns)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Patterns for good night\n",
    "    gn_patterns = [\n",
    "        r'\\bgood\\s*night\\b',\n",
    "        r'\\bgd\\s*night\\b',\n",
    "        r'\\bg\\s*night\\b',\n",
    "        r'\\bnight\\b(?!\\s*(shift|out|club|life))',  # night but not night shift/out/club\n",
    "        r'\\bgn\\b(?!\\s*(shift|out))',  # gn but not GN shift/out\n",
    "        r'\\bgood\\s*nite\\b',\n",
    "        r'\\bgood\\s*noite\\b',\n",
    "        r'\\bnighty\\b',\n",
    "        r'\\bgutten\\s*nacht\\b',  # German\n",
    "        r'\\bbuenas\\s*noches\\b', # Spanish\n",
    "        r'\\bbonne\\s*nuit\\b',    # French\n",
    "        r'\\bsweet\\s*dreams\\b',\n",
    "        r'\\bsleep\\s*well\\b',\n",
    "        r'\\bsleep\\s*tight\\b'\n",
    "    ]\n",
    "    \n",
    "    return any(re.search(pattern, text) for pattern in gn_patterns)\n",
    "\n",
    "# Apply detection functions\n",
    "df['is_good_morning'] = df['text_message'].apply(detect_good_morning)\n",
    "df['is_good_night'] = df['text_message'].apply(detect_good_night)\n",
    "\n",
    "# Filter messages\n",
    "good_morning_msgs = df[df['is_good_morning']].copy()\n",
    "good_night_msgs = df[df['is_good_night']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== GOOD MORNING & GOOD NIGHT ANALYSIS ===\")\n",
    "\n",
    "# Overall statistics\n",
    "total_gm = len(good_morning_msgs)\n",
    "total_gn = len(good_night_msgs)\n",
    "total_messages = len(df)\n",
    "\n",
    "print(f\"Total Good Morning messages: {total_gm:,} ({(total_gm/total_messages)*100:.2f}% of all messages)\")\n",
    "print(f\"Total Good Night messages: {total_gn:,} ({(total_gn/total_messages)*100:.2f}% of all messages)\")\n",
    "\n",
    "# Frequency by user\n",
    "print(\"\\n=== FREQUENCY BY USER ===\")\n",
    "gm_by_user = good_morning_msgs['User'].value_counts()\n",
    "gn_by_user = good_night_msgs['User'].value_counts()\n",
    "\n",
    "print(\"Good Morning messages by user:\")\n",
    "for user in df['User'].unique():\n",
    "    count = gm_by_user.get(user, 0)\n",
    "    percentage = (count / total_gm * 100) if total_gm > 0 else 0\n",
    "    user_total = len(df[df['User'] == user])\n",
    "    user_percentage = (count / user_total * 100) if user_total > 0 else 0\n",
    "    print(f\"  {user}: {count:,} messages ({percentage:.1f}% of all GM, {user_percentage:.2f}% of their messages)\")\n",
    "\n",
    "print(\"\\nGood Night messages by user:\")\n",
    "for user in df['User'].unique():\n",
    "    count = gn_by_user.get(user, 0)\n",
    "    percentage = (count / total_gn * 100) if total_gn > 0 else 0\n",
    "    user_total = len(df[df['User'] == user])\n",
    "    user_percentage = (count / user_total * 100) if user_total > 0 else 0\n",
    "    print(f\"  {user}: {count:,} messages ({percentage:.1f}% of all GN, {user_percentage:.2f}% of their messages)\")\n",
    "\n",
    "# Combined user statistics\n",
    "user_greeting_stats = pd.DataFrame({\n",
    "    'Good Morning': [gm_by_user.get(user, 0) for user in df['User'].unique()],\n",
    "    'Good Night': [gn_by_user.get(user, 0) for user in df['User'].unique()],\n",
    "}, index=df['User'].unique())\n",
    "\n",
    "user_greeting_stats['Total Greetings'] = user_greeting_stats['Good Morning'] + user_greeting_stats['Good Night']\n",
    "user_greeting_stats['GM/GN Ratio'] = user_greeting_stats['Good Morning'] / (user_greeting_stats['Good Night'] + 1)  # +1 to avoid division by zero\n",
    "\n",
    "print(f\"\\nCombined Greeting Statistics:\")\n",
    "print(user_greeting_stats)\n",
    "\n",
    "# Time-based analysis\n",
    "if total_gm > 0:\n",
    "    good_morning_msgs['month_year'] = good_morning_msgs['datetime'].dt.to_period('M')\n",
    "    gm_monthly = good_morning_msgs.groupby('month_year').size()\n",
    "    \n",
    "    # Peak months for good morning\n",
    "    peak_gm_month = gm_monthly.idxmax()\n",
    "    peak_gm_count = gm_monthly.max()\n",
    "    print(f\"\\nPeak Good Morning month: {peak_gm_month} ({peak_gm_count} messages)\")\n",
    "\n",
    "if total_gn > 0:\n",
    "    good_night_msgs['month_year'] = good_night_msgs['datetime'].dt.to_period('M')\n",
    "    gn_monthly = good_night_msgs.groupby('month_year').size()\n",
    "    \n",
    "    # Peak months for good night\n",
    "    peak_gn_month = gn_monthly.idxmax()\n",
    "    peak_gn_count = gn_monthly.max()\n",
    "    print(f\"Peak Good Night month: {peak_gn_month} ({peak_gn_count} messages)\")\n",
    "\n",
    "# Show some sample messages\n",
    "print(f\"\\n=== SAMPLE MESSAGES ===\")\n",
    "if total_gm > 0:\n",
    "    print(\"Sample Good Morning messages:\")\n",
    "    for i, (idx, row) in enumerate(good_morning_msgs.sample(min(5, total_gm)).iterrows(), 1):\n",
    "        print(f\"{i}. {row['User']} ({row['date']}): {row['text_message']}\")\n",
    "\n",
    "if total_gn > 0:\n",
    "    print(\"\\nSample Good Night messages:\")\n",
    "    for i, (idx, row) in enumerate(good_night_msgs.sample(min(5, total_gn)).iterrows(), 1):\n",
    "        print(f\"{i}. {row['User']} ({row['date']}): {row['text_message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. User comparison bar chart\n",
    "user_greeting_stats[['Good Morning', 'Good Night']].plot(kind='bar', ax=axes[0,0], \n",
    "                                                         color=['gold', 'navy'], alpha=0.8)\n",
    "axes[0,0].set_title('Good Morning vs Good Night Messages by User')\n",
    "axes[0,0].set_xlabel('User')\n",
    "axes[0,0].set_ylabel('Number of Messages')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Pie chart for overall distribution\n",
    "greeting_totals = [total_gm, total_gn]\n",
    "greeting_labels = ['Good Morning', 'Good Night']\n",
    "if sum(greeting_totals) > 0:\n",
    "    axes[0,1].pie(greeting_totals, labels=greeting_labels, autopct='%1.1f%%', \n",
    "                  colors=['gold', 'navy'], startangle=90)\n",
    "    axes[0,1].set_title('Overall GM vs GN Distribution')\n",
    "\n",
    "# 3. GM/GN Ratio by user\n",
    "user_greeting_stats['GM/GN Ratio'].plot(kind='bar', ax=axes[0,2], color='purple', alpha=0.7)\n",
    "axes[0,2].set_title('Good Morning to Good Night Ratio by User')\n",
    "axes[0,2].set_xlabel('User')\n",
    "axes[0,2].set_ylabel('GM/GN Ratio')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "axes[0,2].axhline(1, color='red', linestyle='--', alpha=0.7, label='Equal ratio')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# 4. & 5. Time series plots for GM and GN over time\n",
    "if total_gm > 0 and total_gn > 0:\n",
    "    # Combine monthly data for both\n",
    "    all_months = pd.period_range(start=df['datetime'].min(), end=df['datetime'].max(), freq='M')\n",
    "    \n",
    "    # Ensure both series have the same index\n",
    "    gm_monthly_full = gm_monthly.reindex(all_months, fill_value=0)\n",
    "    gn_monthly_full = gn_monthly.reindex(all_months, fill_value=0)\n",
    "    \n",
    "    # Combined time series\n",
    "    combined_monthly = pd.DataFrame({\n",
    "        'Good Morning': gm_monthly_full,\n",
    "        'Good Night': gn_monthly_full\n",
    "    })\n",
    "    \n",
    "    # Plot combined time series\n",
    "    combined_monthly.plot(kind='line', ax=axes[1,0], marker='o', linewidth=2, \n",
    "                         color=['gold', 'navy'], alpha=0.8)\n",
    "    axes[1,0].set_title('Good Morning & Good Night Messages Over Time')\n",
    "    axes[1,0].set_xlabel('Month')\n",
    "    axes[1,0].set_ylabel('Number of Messages')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Individual GM time series\n",
    "    gm_monthly_full.plot(kind='line', ax=axes[1,1], marker='o', linewidth=2, \n",
    "                        color='gold', alpha=0.8)\n",
    "    axes[1,1].set_title('Good Morning Messages Over Time')\n",
    "    axes[1,1].set_xlabel('Month')\n",
    "    axes[1,1].set_ylabel('Number of Messages')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Individual GN time series\n",
    "    gn_monthly_full.plot(kind='line', ax=axes[1,2], marker='o', linewidth=2, \n",
    "                        color='navy', alpha=0.8)\n",
    "    axes[1,2].set_title('Good Night Messages Over Time')\n",
    "    axes[1,2].set_xlabel('Month')\n",
    "    axes[1,2].set_ylabel('Number of Messages')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "elif total_gm > 0:\n",
    "    # Only GM data available\n",
    "    gm_monthly.plot(kind='line', ax=axes[1,0], marker='o', linewidth=2, color='gold')\n",
    "    axes[1,0].set_title('Good Morning Messages Over Time')\n",
    "    axes[1,0].set_xlabel('Month')\n",
    "    axes[1,0].set_ylabel('Number of Messages')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    axes[1,1].set_visible(False)\n",
    "    axes[1,2].set_visible(False)\n",
    "\n",
    "elif total_gn > 0:\n",
    "    # Only GN data available\n",
    "    gn_monthly.plot(kind='line', ax=axes[1,0], marker='o', linewidth=2, color='navy')\n",
    "    axes[1,0].set_title('Good Night Messages Over Time')\n",
    "    axes[1,0].set_xlabel('Month')\n",
    "    axes[1,0].set_ylabel('Number of Messages')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    axes[1,1].set_visible(False)\n",
    "    axes[1,2].set_visible(False)\n",
    "\n",
    "else:\n",
    "    # No greeting data found\n",
    "    for i in range(3):\n",
    "        axes[1,i].text(0.5, 0.5, 'No greeting messages found', \n",
    "                      ha='center', va='center', transform=axes[1,i].transAxes)\n",
    "        axes[1,i].set_xlim(0, 1)\n",
    "        axes[1,i].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interactive Plotly visualization for time series\n",
    "if total_gm > 0 or total_gn > 0:\n",
    "    print(\"\\n=== INTERACTIVE TIME SERIES VISUALIZATION ===\")\n",
    "    \n",
    "    # Create interactive plot\n",
    "    fig_interactive = go.Figure()\n",
    "    \n",
    "    if total_gm > 0:\n",
    "        fig_interactive.add_trace(go.Scatter(\n",
    "            x=[str(date) for date in gm_monthly.index],\n",
    "            y=gm_monthly.values,\n",
    "            mode='lines+markers',\n",
    "            name='Good Morning',\n",
    "            line=dict(color='gold', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ))\n",
    "    \n",
    "    if total_gn > 0:\n",
    "        fig_interactive.add_trace(go.Scatter(\n",
    "            x=[str(date) for date in gn_monthly.index],\n",
    "            y=gn_monthly.values,\n",
    "            mode='lines+markers',\n",
    "            name='Good Night',\n",
    "            line=dict(color='darkblue', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ))\n",
    "    \n",
    "    fig_interactive.update_layout(\n",
    "        title='Good Morning & Good Night Messages Over Time (Interactive)',\n",
    "        xaxis_title='Month',\n",
    "        yaxis_title='Number of Messages',\n",
    "        hovermode='x unified',\n",
    "        width=1000,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_interactive.show()\n",
    "\n",
    "# Additional analysis: Time of day patterns\n",
    "if total_gm > 0:\n",
    "    gm_hours = good_morning_msgs['hour'].value_counts().sort_index()\n",
    "    print(f\"\\n=== GOOD MORNING TIME PATTERNS ===\")\n",
    "    print(\"Most common hours for Good Morning messages:\")\n",
    "    for hour, count in gm_hours.head(10).items():\n",
    "        time_str = f\"{hour:02d}:00\"\n",
    "        percentage = (count / total_gm) * 100\n",
    "        print(f\"  {time_str}: {count:,} messages ({percentage:.1f}%)\")\n",
    "\n",
    "if total_gn > 0:\n",
    "    gn_hours = good_night_msgs['hour'].value_counts().sort_index()\n",
    "    print(f\"\\n=== GOOD NIGHT TIME PATTERNS ===\")\n",
    "    print(\"Most common hours for Good Night messages:\")\n",
    "    for hour, count in gn_hours.head(10).items():\n",
    "        time_str = f\"{hour:02d}:00\"\n",
    "        percentage = (count / total_gn) * 100\n",
    "        print(f\"  {time_str}: {count:,} messages ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251642cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly and weekly patterns visualization\n",
    "if total_gm > 0 or total_gn > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Hour distribution for Good Morning\n",
    "    if total_gm > 0:\n",
    "        gm_hours = good_morning_msgs['hour'].value_counts().sort_index()\n",
    "        gm_hours.plot(kind='bar', ax=axes[0,0], color='gold', alpha=0.8)\n",
    "        axes[0,0].set_title('Good Morning Messages by Hour of Day')\n",
    "        axes[0,0].set_xlabel('Hour')\n",
    "        axes[0,0].set_ylabel('Number of Messages')\n",
    "        axes[0,0].tick_params(axis='x', rotation=0)\n",
    "    else:\n",
    "        axes[0,0].text(0.5, 0.5, 'No Good Morning messages found', \n",
    "                      ha='center', va='center', transform=axes[0,0].transAxes)\n",
    "    \n",
    "    # Hour distribution for Good Night\n",
    "    if total_gn > 0:\n",
    "        gn_hours = good_night_msgs['hour'].value_counts().sort_index()\n",
    "        gn_hours.plot(kind='bar', ax=axes[0,1], color='navy', alpha=0.8)\n",
    "        axes[0,1].set_title('Good Night Messages by Hour of Day')\n",
    "        axes[0,1].set_xlabel('Hour')\n",
    "        axes[0,1].set_ylabel('Number of Messages')\n",
    "        axes[0,1].tick_params(axis='x', rotation=0)\n",
    "    else:\n",
    "        axes[0,1].text(0.5, 0.5, 'No Good Night messages found', \n",
    "                      ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "    \n",
    "    # Day of week patterns for Good Morning\n",
    "    if total_gm > 0:\n",
    "        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        gm_weekdays = good_morning_msgs['weekday'].value_counts().reindex(weekday_order, fill_value=0)\n",
    "        gm_weekdays.plot(kind='bar', ax=axes[1,0], color='gold', alpha=0.8)\n",
    "        axes[1,0].set_title('Good Morning Messages by Day of Week')\n",
    "        axes[1,0].set_xlabel('Day of Week')\n",
    "        axes[1,0].set_ylabel('Number of Messages')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Day of week patterns for Good Night\n",
    "    if total_gn > 0:\n",
    "        gn_weekdays = good_night_msgs['weekday'].value_counts().reindex(weekday_order, fill_value=0)\n",
    "        gn_weekdays.plot(kind='bar', ax=axes[1,1], color='navy', alpha=0.8)\n",
    "        axes[1,1].set_title('Good Night Messages by Day of Week')\n",
    "        axes[1,1].set_xlabel('Day of Week')\n",
    "        axes[1,1].set_ylabel('Number of Messages')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897354fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weekly patterns analysis\n",
    "if total_gm > 0:\n",
    "    print(f\"\\n=== WEEKLY PATTERNS FOR GOOD MORNING ===\")\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    gm_weekday_stats = good_morning_msgs['weekday'].value_counts().reindex(weekday_order, fill_value=0)\n",
    "    \n",
    "    for day, count in gm_weekday_stats.items():\n",
    "        percentage = (count / total_gm) * 100\n",
    "        print(f\"  {day}: {count:,} messages ({percentage:.1f}%)\")\n",
    "    \n",
    "    most_gm_day = gm_weekday_stats.idxmax()\n",
    "    least_gm_day = gm_weekday_stats.idxmin()\n",
    "    print(f\"  Most GM messages: {most_gm_day} ({gm_weekday_stats.max()} messages)\")\n",
    "    print(f\"  Least GM messages: {least_gm_day} ({gm_weekday_stats.min()} messages)\")\n",
    "\n",
    "if total_gn > 0:\n",
    "    print(f\"\\n=== WEEKLY PATTERNS FOR GOOD NIGHT ===\")\n",
    "    gn_weekday_stats = good_night_msgs['weekday'].value_counts().reindex(weekday_order, fill_value=0)\n",
    "    \n",
    "    for day, count in gn_weekday_stats.items():\n",
    "        percentage = (count / total_gn) * 100\n",
    "        print(f\"  {day}: {count:,} messages ({percentage:.1f}%)\")\n",
    "    \n",
    "    most_gn_day = gn_weekday_stats.idxmax()\n",
    "    least_gn_day = gn_weekday_stats.idxmin()\n",
    "    print(f\"  Most GN messages: {most_gn_day} ({gn_weekday_stats.max()} messages)\")\n",
    "    print(f\"  Least GN messages: {least_gn_day} ({gn_weekday_stats.min()} messages)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ff4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Advanced insights\n",
    "print(f\"\\n=== ADVANCED GREETING INSIGHTS ===\")\n",
    "\n",
    "# Calculate greeting consistency (how often users send both GM and GN)\n",
    "if total_gm > 0 and total_gn > 0:\n",
    "    gm_days = set(good_morning_msgs['date'])\n",
    "    gn_days = set(good_night_msgs['date'])\n",
    "    both_days = gm_days.intersection(gn_days)\n",
    "    \n",
    "    print(f\"Days with Good Morning messages: {len(gm_days):,}\")\n",
    "    print(f\"Days with Good Night messages: {len(gn_days):,}\")\n",
    "    print(f\"Days with both GM and GN: {len(both_days):,}\")\n",
    "    \n",
    "    if len(gm_days.union(gn_days)) > 0:\n",
    "        consistency_rate = len(both_days) / len(gm_days.union(gn_days))\n",
    "        print(f\"Greeting consistency rate: {consistency_rate:.1%}\")\n",
    "\n",
    "# User-specific patterns\n",
    "for user in df['User'].unique():\n",
    "    user_gm = len(good_morning_msgs[good_morning_msgs['User'] == user])\n",
    "    user_gn = len(good_night_msgs[good_night_msgs['User'] == user])\n",
    "    user_total = len(df[df['User'] == user])\n",
    "    \n",
    "    print(f\"\\n{user} greeting patterns:\")\n",
    "    print(f\"  Good Morning: {user_gm:,} ({(user_gm/user_total)*100:.2f}% of their messages)\")\n",
    "    print(f\"  Good Night: {user_gn:,} ({(user_gn/user_total)*100:.2f}% of their messages)\")\n",
    "    \n",
    "    if user_gm > 0 and user_gn > 0:\n",
    "        user_gm_msgs = good_morning_msgs[good_morning_msgs['User'] == user]\n",
    "        user_gn_msgs = good_night_msgs[good_night_msgs['User'] == user]\n",
    "        \n",
    "        # Average time for greetings\n",
    "        avg_gm_hour = user_gm_msgs['hour'].mean()\n",
    "        avg_gn_hour = user_gn_msgs['hour'].mean()\n",
    "        \n",
    "        print(f\"  Average GM time: {avg_gm_hour:.1f}:00 ({int(avg_gm_hour):02d}:{int((avg_gm_hour % 1) * 60):02d})\")\n",
    "        print(f\"  Average GN time: {avg_gn_hour:.1f}:00 ({int(avg_gn_hour):02d}:{int((avg_gn_hour % 1) * 60):02d})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GOOD MORNING & GOOD NIGHT ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cfdec4",
   "metadata": {},
   "source": [
    "## 9. Named Entity Recognition Analysis\n",
    "\n",
    "This section identifies and analyzes the most frequently mentioned named entities (people, places, organizations, etc.) in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"‚úÖ spaCy English model installed and loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98734455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    \"\"\"Extract named entities from text using spaCy\"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp(str(text))\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            # Filter out very short entities and common false positives\n",
    "            if len(ent.text.strip()) > 2 and ent.text.strip().lower() not in ['you', 'me', 'i', 'we', 'us', 'them']:\n",
    "                entities.append({\n",
    "                    'text': ent.text.strip(),\n",
    "                    'label': ent.label_,\n",
    "                    'description': spacy.explain(ent.label_)\n",
    "                })\n",
    "        \n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "print(\"=== NAMED ENTITY RECOGNITION ANALYSIS ===\")\n",
    "print(\"Processing messages for named entities... This may take a few minutes for large datasets.\")\n",
    "\n",
    "# Extract entities from all messages\n",
    "all_entities = []\n",
    "user_entities = defaultdict(list)\n",
    "entity_by_type = defaultdict(list)\n",
    "\n",
    "# Process messages in batches to show progress\n",
    "batch_size = 1000\n",
    "total_messages = len(df)\n",
    "processed = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    entities = extract_entities(row['text_message'])\n",
    "    \n",
    "    for entity in entities:\n",
    "        all_entities.append(entity)\n",
    "        user_entities[row['User']].append(entity)\n",
    "        entity_by_type[entity['label']].append(entity)\n",
    "    \n",
    "    processed += 1\n",
    "    if processed % batch_size == 0:\n",
    "        print(f\"Processed {processed:,}/{total_messages:,} messages ({(processed/total_messages)*100:.1f}%)\")\n",
    "\n",
    "print(f\"‚úÖ Completed processing {total_messages:,} messages!\")\n",
    "\n",
    "# Count entities\n",
    "entity_counts = Counter([ent['text'].lower() for ent in all_entities])\n",
    "entity_type_counts = Counter([ent['label'] for ent in all_entities])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n=== ENTITY STATISTICS ===\")\n",
    "print(f\"Total entities found: {len(all_entities):,}\")\n",
    "print(f\"Unique entities: {len(entity_counts):,}\")\n",
    "print(f\"Entity types found: {len(entity_type_counts)}\")\n",
    "\n",
    "# Show entity type distribution\n",
    "print(f\"\\n=== ENTITY TYPES DISTRIBUTION ===\")\n",
    "for entity_type, count in entity_type_counts.most_common():\n",
    "    description = spacy.explain(entity_type) or entity_type\n",
    "    percentage = (count / len(all_entities)) * 100\n",
    "    print(f\"{entity_type} ({description}): {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Most mentioned entities overall\n",
    "print(f\"\\n=== TOP 30 MOST MENTIONED ENTITIES (OVERALL) ===\")\n",
    "for i, (entity, count) in enumerate(entity_counts.most_common(30), 1):\n",
    "    # Find the entity type for this entity\n",
    "    entity_types = set()\n",
    "    for ent in all_entities:\n",
    "        if ent['text'].lower() == entity:\n",
    "            entity_types.add(ent['label'])\n",
    "    \n",
    "    type_str = ', '.join(entity_types)\n",
    "    print(f\"{i:2d}. {entity.title():<20} : {count:,} mentions ({type_str})\")\n",
    "\n",
    "# Entities by user\n",
    "print(f\"\\n=== TOP ENTITIES BY USER ===\")\n",
    "for user in df['User'].unique():\n",
    "    user_entity_counts = Counter([ent['text'].lower() for ent in user_entities[user]])\n",
    "    print(f\"\\n{user} - Top 15 entities:\")\n",
    "    \n",
    "    for i, (entity, count) in enumerate(user_entity_counts.most_common(15), 1):\n",
    "        percentage = (count / len(user_entities[user])) * 100 if len(user_entities[user]) > 0 else 0\n",
    "        print(f\"  {i:2d}. {entity.title():<20} : {count:,} mentions ({percentage:.1f}%)\")\n",
    "\n",
    "# Show entities by category\n",
    "print(f\"\\n=== ENTITIES BY CATEGORY ===\")\n",
    "important_types = ['PERSON', 'GPE', 'ORG', 'PRODUCT', 'EVENT', 'FAC', 'LOC']\n",
    "\n",
    "for entity_type in important_types:\n",
    "    if entity_type in entity_type_counts:\n",
    "        type_entities = [ent['text'].lower() for ent in all_entities if ent['label'] == entity_type]\n",
    "        type_entity_counts = Counter(type_entities)\n",
    "        \n",
    "        description = spacy.explain(entity_type) or entity_type\n",
    "        print(f\"\\n{entity_type} ({description}) - Top 10:\")\n",
    "        \n",
    "        for i, (entity, count) in enumerate(type_entity_counts.most_common(10), 1):\n",
    "            print(f\"  {i:2d}. {entity.title():<25} : {count:,} mentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc26da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Word Cloud Visualizations\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NAMED ENTITY WORD CLOUD VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(entity_counts) > 0:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Overall entities word cloud\n",
    "    overall_wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        relative_scaling=0.5,\n",
    "        random_state=42\n",
    "    ).generate_from_frequencies(entity_counts)\n",
    "    \n",
    "    axes.imshow(overall_wordcloud, interpolation='bilinear')\n",
    "    axes.set_title('All Named Entities (Combined)', fontsize=16, fontweight='bold')\n",
    "    axes.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54319f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual word clouds for each user    \n",
    "if len(entity_counts) > 0:\n",
    "    users = list(df['User'].unique())\n",
    "    \n",
    "    # Create subplot for user comparison\n",
    "    fig, axes = plt.subplots(1, len(users), figsize=(12 * len(users), 8))\n",
    "    \n",
    "    # Handle single user case\n",
    "    if len(users) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, user in enumerate(users):\n",
    "        user_entity_counts = Counter([ent['text'].lower() for ent in user_entities[user]])\n",
    "        \n",
    "        if len(user_entity_counts) > 0:\n",
    "            user_wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                background_color='white',\n",
    "                colormap='plasma' if i == 0 else 'cool',\n",
    "                max_words=50,\n",
    "                relative_scaling=0.5,\n",
    "                random_state=42\n",
    "            ).generate_from_frequencies(user_entity_counts)\n",
    "            \n",
    "            axes[i].imshow(user_wordcloud, interpolation='bilinear')\n",
    "            axes[i].set_title(f'{user} - Named Entities', fontsize=16, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f'No entities found for {user}', \n",
    "                            ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f'{user} - Named Entities')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity type-specific word clouds\n",
    "if len(entity_counts) > 0:\n",
    "    # Entity type-specific word clouds\n",
    "    important_types = ['PERSON', 'GPE', 'ORG']\n",
    "    available_types = [t for t in important_types if t in entity_type_counts]\n",
    "    \n",
    "    if available_types:\n",
    "        fig, axes = plt.subplots(1, len(available_types), figsize=(8 * len(available_types), 8))\n",
    "        \n",
    "        # Handle single type case\n",
    "        if len(available_types) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        colors = ['Set1', 'Set2', 'Set3']\n",
    "        \n",
    "        for i, entity_type in enumerate(available_types):\n",
    "            type_entities = [ent['text'].lower() for ent in all_entities if ent['label'] == entity_type]\n",
    "            type_entity_counts = Counter(type_entities)\n",
    "            \n",
    "            if len(type_entity_counts) > 0:\n",
    "                type_wordcloud = WordCloud(\n",
    "                    width=800, \n",
    "                    height=400, \n",
    "                    background_color='white',\n",
    "                    colormap=colors[i % len(colors)],\n",
    "                    max_words=30,\n",
    "                    relative_scaling=0.5,\n",
    "                    random_state=42\n",
    "                ).generate_from_frequencies(type_entity_counts)\n",
    "                \n",
    "                description = spacy.explain(entity_type) or entity_type\n",
    "                axes[i].imshow(type_wordcloud, interpolation='bilinear')\n",
    "                axes[i].set_title(f'{entity_type} - {description}', fontsize=14, fontweight='bold')\n",
    "                axes[i].axis('off')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, f'No {entity_type} entities found', \n",
    "                            ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].set_title(f'{entity_type} Entities')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No major entity types found for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity frequency bar charts\n",
    "if len(all_entities) > 0:\n",
    "    # Create entity frequency bar charts\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    # Top entities overall\n",
    "    top_entities = dict(entity_counts.most_common(20))\n",
    "    if top_entities:\n",
    "        axes[0,0].barh(range(len(top_entities)), list(top_entities.values()), color='skyblue')\n",
    "        axes[0,0].set_title('Top 20 Most Mentioned Entities (Overall)')\n",
    "        axes[0,0].set_xlabel('Mention Count')\n",
    "        axes[0,0].set_ylabel('Entities')\n",
    "        axes[0,0].set_yticks(range(len(top_entities)))\n",
    "        axes[0,0].set_yticklabels([entity.title() for entity in top_entities.keys()])\n",
    "        axes[0,0].invert_yaxis()\n",
    "    \n",
    "    # Entity types distribution\n",
    "    if entity_type_counts:\n",
    "        axes[0,1].bar(range(len(entity_type_counts)), list(entity_type_counts.values()), \n",
    "                     color='lightgreen', alpha=0.8)\n",
    "        axes[0,1].set_title('Entity Types Distribution')\n",
    "        axes[0,1].set_xlabel('Entity Type')\n",
    "        axes[0,1].set_ylabel('Count')\n",
    "        axes[0,1].set_xticks(range(len(entity_type_counts)))\n",
    "        axes[0,1].set_xticklabels(list(entity_type_counts.keys()), rotation=45)\n",
    "    \n",
    "    # User comparison\n",
    "    if len(users) >= 2:\n",
    "        user1_entities = Counter([ent['text'].lower() for ent in user_entities[users[0]]])\n",
    "        user2_entities = Counter([ent['text'].lower() for ent in user_entities[users[1]]])\n",
    "        \n",
    "        # Get common top entities for comparison\n",
    "        all_top_entities = set(list(user1_entities.keys())[:10] + list(user2_entities.keys())[:10])\n",
    "        common_entities = list(all_top_entities)[:15]\n",
    "        \n",
    "        user1_counts = [user1_entities.get(entity, 0) for entity in common_entities]\n",
    "        user2_counts = [user2_entities.get(entity, 0) for entity in common_entities]\n",
    "        \n",
    "        x = np.arange(len(common_entities))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1,0].bar(x - width/2, user1_counts, width, label=users[0], alpha=0.8)\n",
    "        axes[1,0].bar(x + width/2, user2_counts, width, label=users[1], alpha=0.8)\n",
    "        axes[1,0].set_title('Top Entities Comparison by User')\n",
    "        axes[1,0].set_xlabel('Entities')\n",
    "        axes[1,0].set_ylabel('Mention Count')\n",
    "        axes[1,0].set_xticks(x)\n",
    "        axes[1,0].set_xticklabels([entity.title()[:10] for entity in common_entities], rotation=45)\n",
    "        axes[1,0].legend()\n",
    "    \n",
    "    # Timeline of entity mentions (monthly)\n",
    "    if len(all_entities) > 0:\n",
    "        # Create entity timeline\n",
    "        entity_timeline = defaultdict(int)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if row['text_message'] and not pd.isna(row['text_message']):\n",
    "                entities_in_msg = extract_entities(row['text_message'])\n",
    "                if entities_in_msg:\n",
    "                    month_year = row['datetime'].strftime('%Y-%m')\n",
    "                    entity_timeline[month_year] += len(entities_in_msg)\n",
    "        \n",
    "        if entity_timeline:\n",
    "            timeline_sorted = dict(sorted(entity_timeline.items()))\n",
    "            axes[1,1].plot(list(timeline_sorted.keys()), list(timeline_sorted.values()), \n",
    "                          marker='o', linewidth=2, markersize=4)\n",
    "            axes[1,1].set_title('Named Entity Mentions Over Time')\n",
    "            axes[1,1].set_xlabel('Month')\n",
    "            axes[1,1].set_ylabel('Total Entity Mentions')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No named entities found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb672ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced entity analysis\n",
    "print(f\"\\n=== ADVANCED ENTITY INSIGHTS ===\")\n",
    "\n",
    "if len(all_entities) > 0:\n",
    "    # Entity diversity by user\n",
    "    for user in users:\n",
    "        user_entity_count = len(user_entities[user])\n",
    "        user_unique_entities = len(set([ent['text'].lower() for ent in user_entities[user]]))\n",
    "        diversity = user_unique_entities / user_entity_count if user_entity_count > 0 else 0\n",
    "        \n",
    "        print(f\"{user} entity usage:\")\n",
    "        print(f\"  Total entity mentions: {user_entity_count:,}\")\n",
    "        print(f\"  Unique entities mentioned: {user_unique_entities:,}\")\n",
    "        print(f\"  Entity diversity ratio: {diversity:.3f}\")\n",
    "    \n",
    "    # Most mentioned entity per category\n",
    "    print(f\"\\n=== TOP ENTITY PER CATEGORY ===\")\n",
    "    for entity_type in important_types:\n",
    "        if entity_type in entity_type_counts:\n",
    "            type_entities = [ent['text'].lower() for ent in all_entities if ent['label'] == entity_type]\n",
    "            if type_entities:\n",
    "                top_entity = Counter(type_entities).most_common(1)[0]\n",
    "                description = spacy.explain(entity_type) or entity_type\n",
    "                print(f\"{entity_type} ({description}): '{top_entity[0].title()}' ({top_entity[1]} mentions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NAMED ENTITY ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431cb3f",
   "metadata": {},
   "source": [
    "## 10. Laughter Analysis üòÇ\n",
    "\n",
    "This section analyzes laughter patterns including \"lol\", \"lmao\", \"lmfao\", and \"ha\" chains to discover who laughed the most and when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62096a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laughter Analysis - Detecting various forms of laughter\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def detect_laughter_patterns(text):\n",
    "    \"\"\"Detect various laughter patterns in text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    laughter_patterns = {}\n",
    "    \n",
    "    # LOL variants\n",
    "    lol_pattern = r'\\b(lol|lols|lolol|lololo)\\b'\n",
    "    lol_matches = re.findall(lol_pattern, text)\n",
    "    laughter_patterns['lol'] = len(lol_matches)\n",
    "    \n",
    "    # LMAO variants\n",
    "    lmao_pattern = r'\\b(lmao|lmaoo|lmaooo)\\b'\n",
    "    lmao_matches = re.findall(lmao_pattern, text)\n",
    "    laughter_patterns['lmao'] = len(lmao_matches)\n",
    "    \n",
    "    # LMFAO variants\n",
    "    lmfao_pattern = r'\\b(lmfao|lmfaoo|lmfaooo)\\b'\n",
    "    lmfao_matches = re.findall(lmfao_pattern, text)\n",
    "    laughter_patterns['lmfao'] = len(lmfao_matches)\n",
    "    \n",
    "    # HA chains (haha, hahaha, hahahaha, etc.)\n",
    "    ha_pattern = r'\\b(ha){2,}\\b'  # At least 2 consecutive \"ha\"s\n",
    "    ha_matches = re.findall(ha_pattern, text)\n",
    "    laughter_patterns['ha_chains'] = len(ha_matches)\n",
    "    \n",
    "    # Get the actual ha chain lengths for more detailed analysis\n",
    "    ha_chain_lengths = []\n",
    "    for match in re.finditer(r'\\b(ha)+\\b', text):\n",
    "        chain_length = len(match.group()) // 2  # Each \"ha\" is 2 characters\n",
    "        if chain_length >= 2:  # Only count chains of 2 or more\n",
    "            ha_chain_lengths.append(chain_length)\n",
    "    \n",
    "    laughter_patterns['ha_chain_lengths'] = ha_chain_lengths\n",
    "    laughter_patterns['max_ha_chain'] = max(ha_chain_lengths) if ha_chain_lengths else 0\n",
    "    \n",
    "    # ROFL and other variants\n",
    "    rofl_pattern = r'\\b(rofl|roflmao|rotfl)\\b'\n",
    "    rofl_matches = re.findall(rofl_pattern, text)\n",
    "    laughter_patterns['rofl'] = len(rofl_matches)\n",
    "    \n",
    "    # Calculate total laughter instances\n",
    "    laughter_patterns['total_laughter'] = (\n",
    "        laughter_patterns['lol'] + \n",
    "        laughter_patterns['lmao'] + \n",
    "        laughter_patterns['lmfao'] + \n",
    "        laughter_patterns['ha_chains'] + \n",
    "        laughter_patterns['rofl']\n",
    "    )\n",
    "    \n",
    "    return laughter_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== LAUGHTER ANALYSIS ===\")\n",
    "print(\"Analyzing laughter patterns in messages...\")\n",
    "\n",
    "# Apply laughter detection to all messages\n",
    "laughter_data = []\n",
    "user_laughter = defaultdict(lambda: defaultdict(int))\n",
    "monthly_laughter = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    laughter = detect_laughter_patterns(row['text_message'])\n",
    "    \n",
    "    if laughter['total_laughter'] > 0:\n",
    "        # Store message info with laughter data\n",
    "        laughter_entry = {\n",
    "            'message_id': idx,\n",
    "            'user': row['User'],\n",
    "            'date': row['date'],\n",
    "            'datetime': row['datetime'],\n",
    "            'message': row['text_message'],\n",
    "            **laughter\n",
    "        }\n",
    "        laughter_data.append(laughter_entry)\n",
    "        \n",
    "        # Aggregate by user\n",
    "        for pattern in ['lol', 'lmao', 'lmfao', 'ha_chains', 'rofl', 'total_laughter']:\n",
    "            user_laughter[row['User']][pattern] += laughter[pattern]\n",
    "        \n",
    "        # Add max ha chain length for user\n",
    "        if laughter['max_ha_chain'] > user_laughter[row['User']]['max_ha_chain']:\n",
    "            user_laughter[row['User']]['max_ha_chain'] = laughter['max_ha_chain']\n",
    "        \n",
    "        # Aggregate by month\n",
    "        month_key = row['datetime'].strftime('%Y-%m')\n",
    "        for pattern in ['lol', 'lmao', 'lmfao', 'ha_chains', 'rofl', 'total_laughter']:\n",
    "            monthly_laughter[month_key][pattern] += laughter[pattern]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39547205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "laughter_df = pd.DataFrame(laughter_data)\n",
    "\n",
    "print(f\"Messages with laughter: {len(laughter_df):,}\")\n",
    "print(f\"Total laughter instances: {sum(entry['total_laughter'] for entry in laughter_data):,}\")\n",
    "\n",
    "# Overall laughter statistics\n",
    "print(f\"\\n=== OVERALL LAUGHTER STATISTICS ===\")\n",
    "total_stats = {\n",
    "    'lol': sum(entry['lol'] for entry in laughter_data),\n",
    "    'lmao': sum(entry['lmao'] for entry in laughter_data),\n",
    "    'lmfao': sum(entry['lmfao'] for entry in laughter_data),\n",
    "    'ha_chains': sum(entry['ha_chains'] for entry in laughter_data),\n",
    "    'rofl': sum(entry['rofl'] for entry in laughter_data),\n",
    "}\n",
    "\n",
    "for pattern, count in sorted(total_stats.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / sum(total_stats.values())) * 100\n",
    "    print(f\"{pattern.upper():<10}: {count:>5,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Laughter by user\n",
    "print(f\"\\n=== LAUGHTER BY USER ===\")\n",
    "for user in df['User'].unique():\n",
    "    user_total = user_laughter[user]['total_laughter']\n",
    "    user_messages = len(df[df['User'] == user])\n",
    "    laughter_rate = (user_total / user_messages) * 100 if user_messages > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{user}:\")\n",
    "    print(f\"  Total laughter: {user_total:,}\")\n",
    "    print(f\"  Laughter rate: {laughter_rate:.2f}% of messages\")\n",
    "    print(f\"  Longest ha chain: {'ha' * user_laughter[user]['max_ha_chain'] if user_laughter[user]['max_ha_chain'] > 0 else 'None'} ({user_laughter[user]['max_ha_chain']} has)\")\n",
    "    \n",
    "    for pattern in ['lol', 'lmao', 'lmfao', 'ha_chains', 'rofl']:\n",
    "        count = user_laughter[user][pattern]\n",
    "        if count > 0:\n",
    "            pattern_pct = (count / user_total) * 100 if user_total > 0 else 0\n",
    "            print(f\"    {pattern}: {count:,} ({pattern_pct:.1f}%)\")\n",
    "\n",
    "# Find the funniest messages (most laughter in one message)\n",
    "if len(laughter_df) > 0:\n",
    "    print(f\"\\n=== FUNNIEST MESSAGES (Most Laughter) ===\")\n",
    "    funniest = laughter_df.nlargest(10, 'total_laughter')\n",
    "    \n",
    "    for i, (idx, row) in enumerate(funniest.iterrows(), 1):\n",
    "        preview = str(row['message'])[:100] + \"...\" if len(str(row['message'])) > 100 else str(row['message'])\n",
    "        print(f\"{i:2d}. {row['user']} - {row['date']} ({row['total_laughter']} laughs)\")\n",
    "        print(f\"    {preview}\")\n",
    "        print()\n",
    "\n",
    "# Peak laughter periods\n",
    "if monthly_laughter:\n",
    "    print(f\"\\n=== PEAK LAUGHTER PERIODS ===\")\n",
    "    monthly_totals = {month: data['total_laughter'] for month, data in monthly_laughter.items()}\n",
    "    sorted_months = sorted(monthly_totals.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top 10 funniest months:\")\n",
    "    for i, (month, total) in enumerate(sorted_months[:10], 1):\n",
    "        print(f\"{i:2d}. {month}: {total:,} laughs\")\n",
    "\n",
    "# Sample messages for each laughter type\n",
    "print(f\"\\n=== SAMPLE MESSAGES BY LAUGHTER TYPE ===\")\n",
    "for pattern in ['lol', 'lmao', 'lmfao', 'ha_chains']:\n",
    "    pattern_messages = laughter_df[laughter_df[pattern] > 0]\n",
    "    if len(pattern_messages) > 0:\n",
    "        print(f\"\\nSample {pattern.upper()} messages:\")\n",
    "        samples = pattern_messages.sample(min(3, len(pattern_messages)))\n",
    "        for i, (idx, row) in enumerate(samples.iterrows(), 1):\n",
    "            preview = str(row['message'])[:120] + \"...\" if len(str(row['message'])) > 120 else str(row['message'])\n",
    "            print(f\"{i}. {row['user']} ({row['date']}): {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Laughter Visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERACTIVE LAUGHTER VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(laughter_df) > 0:\n",
    "    \n",
    "    # 1. Laughter patterns comparison by user\n",
    "    user_laughter_data = []\n",
    "    users = list(df['User'].unique())\n",
    "    \n",
    "    for user in users:\n",
    "        user_data = {\n",
    "            'User': user,\n",
    "            'LOL': user_laughter[user]['lol'],\n",
    "            'LMAO': user_laughter[user]['lmao'], \n",
    "            'LMFAO': user_laughter[user]['lmfao'],\n",
    "            'HA Chains': user_laughter[user]['ha_chains'],\n",
    "            'ROFL': user_laughter[user]['rofl'],\n",
    "            'Total': user_laughter[user]['total_laughter']\n",
    "        }\n",
    "        user_laughter_data.append(user_data)\n",
    "    \n",
    "    user_df = pd.DataFrame(user_laughter_data)\n",
    "    \n",
    "    # Interactive bar chart comparing laughter types by user\n",
    "    fig1 = px.bar(\n",
    "        user_df.melt(id_vars=['User'], value_vars=['LOL', 'LMAO', 'LMFAO', 'HA Chains', 'ROFL'], \n",
    "                     var_name='Laughter Type', value_name='Count'),\n",
    "        x='User', y='Count', color='Laughter Type',\n",
    "        title='Laughter Patterns by User (Interactive)',\n",
    "        labels={'Count': 'Number of Instances'},\n",
    "        color_discrete_sequence=px.colors.qualitative.Set2\n",
    "    )\n",
    "    fig1.update_layout(height=500, showlegend=True)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Laughter over time - Monthly trends\n",
    "    if monthly_laughter:\n",
    "        monthly_df_data = []\n",
    "        for month, data in monthly_laughter.items():\n",
    "            monthly_df_data.append({\n",
    "                'Month': month,\n",
    "                'LOL': data['lol'],\n",
    "                'LMAO': data['lmao'],\n",
    "                'LMFAO': data['lmfao'],\n",
    "                'HA Chains': data['ha_chains'],\n",
    "                'ROFL': data['rofl'],\n",
    "                'Total': data['total_laughter']\n",
    "            })\n",
    "        \n",
    "        monthly_df = pd.DataFrame(monthly_df_data).sort_values('Month')\n",
    "        \n",
    "        # Line chart showing laughter trends over time\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "        patterns = ['LOL', 'LMAO', 'LMFAO', 'HA Chains', 'ROFL']\n",
    "        \n",
    "        for i, pattern in enumerate(patterns):\n",
    "            fig2.add_trace(go.Scatter(\n",
    "                x=monthly_df['Month'],\n",
    "                y=monthly_df[pattern],\n",
    "                mode='lines+markers',\n",
    "                name=pattern,\n",
    "                line=dict(color=colors[i], width=3),\n",
    "                marker=dict(size=8)\n",
    "            ))\n",
    "        \n",
    "        fig2.update_layout(\n",
    "            title='Laughter Trends Over Time (Interactive)',\n",
    "            xaxis_title='Month',\n",
    "            yaxis_title='Number of Instances',\n",
    "            hovermode='x unified',\n",
    "            height=500,\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig2.show()\n",
    "        \n",
    "        # Total laughter trend\n",
    "        fig3 = px.line(\n",
    "            monthly_df, x='Month', y='Total',\n",
    "            title='Total Laughter Over Time',\n",
    "            labels={'Total': 'Total Laughter Instances'},\n",
    "            markers=True\n",
    "        )\n",
    "        fig3.update_traces(line_color='#FF6B6B', line_width=4, marker_size=10)\n",
    "        fig3.update_layout(height=400)\n",
    "        fig3.show()\n",
    "    \n",
    "    # 3. Laughter intensity analysis (ha chain lengths)\n",
    "    if len(laughter_df) > 0:\n",
    "        ha_chain_data = []\n",
    "        for idx, row in laughter_df.iterrows():\n",
    "            if row['ha_chain_lengths']:\n",
    "                for chain_length in row['ha_chain_lengths']:\n",
    "                    ha_chain_data.append({\n",
    "                        'User': row['user'],\n",
    "                        'Chain Length': chain_length,\n",
    "                        'Date': row['date'],\n",
    "                        'Month': row['datetime'].strftime('%Y-%m')\n",
    "                    })\n",
    "        \n",
    "        if ha_chain_data:\n",
    "            ha_df = pd.DataFrame(ha_chain_data)\n",
    "            \n",
    "            # Histogram of ha chain lengths by user\n",
    "            fig4 = px.histogram(\n",
    "                ha_df, x='Chain Length', color='User',\n",
    "                title='HA Chain Length Distribution by User',\n",
    "                labels={'Chain Length': 'Number of \"HA\"s in Chain'},\n",
    "                nbins=20,\n",
    "                opacity=0.7\n",
    "            )\n",
    "            fig4.update_layout(height=400)\n",
    "            fig4.show()\n",
    "            \n",
    "            # Box plot of ha chain lengths\n",
    "            fig5 = px.box(\n",
    "                ha_df, x='User', y='Chain Length',\n",
    "                title='HA Chain Length Distribution (Box Plot)',\n",
    "                labels={'Chain Length': 'Number of \"HA\"s in Chain'}\n",
    "            )\n",
    "            fig5.update_layout(height=400)\n",
    "            fig5.show()\n",
    "    \n",
    "    # 4. Laughter rate vs total messages\n",
    "    user_stats = []\n",
    "    for user in users:\n",
    "        user_total_msgs = len(df[df['User'] == user])\n",
    "        user_total_laughs = user_laughter[user]['total_laughter']\n",
    "        laughter_rate = (user_total_laughs / user_total_msgs) * 100 if user_total_msgs > 0 else 0\n",
    "        \n",
    "        user_stats.append({\n",
    "            'User': user,\n",
    "            'Total Messages': user_total_msgs,\n",
    "            'Total Laughter': user_total_laughs,\n",
    "            'Laughter Rate (%)': laughter_rate\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(user_stats)\n",
    "    \n",
    "    # Scatter plot: Total Messages vs Laughter Rate\n",
    "    fig6 = px.scatter(\n",
    "        stats_df, x='Total Messages', y='Laughter Rate (%)',\n",
    "        size='Total Laughter', color='User', hover_name='User',\n",
    "        title='Message Volume vs Laughter Rate',\n",
    "        labels={'Laughter Rate (%)': 'Laughter Rate (% of messages)'}\n",
    "    )\n",
    "    fig6.update_layout(height=400)\n",
    "    fig6.show()\n",
    "    \n",
    "    # 5. Daily laughter patterns (if data available)\n",
    "    if len(laughter_df) > 0:\n",
    "        # Group by date to see daily patterns\n",
    "        daily_laughter = laughter_df.groupby('date')['total_laughter'].sum().reset_index()\n",
    "        daily_laughter['date'] = pd.to_datetime(daily_laughter['date'])\n",
    "        \n",
    "        # Get top 20 funniest days\n",
    "        top_funny_days = daily_laughter.nlargest(20, 'total_laughter')\n",
    "    \n",
    "        fig7 = px.bar(\n",
    "            top_funny_days, x='date', y='total_laughter',\n",
    "            title='Top 20 Funniest Days',\n",
    "            labels={'total_laughter': 'Total Laughter Instances', 'date': 'Date'}\n",
    "        )\n",
    "        fig7.update_layout(height=500, xaxis_tickangle=-45)\n",
    "        fig7.show()\n",
    "    \n",
    "    # 6. Pie chart of laughter distribution\n",
    "    total_by_type = {\n",
    "        'LOL': sum(user_laughter[user]['lol'] for user in users),\n",
    "        'LMAO': sum(user_laughter[user]['lmao'] for user in users),\n",
    "        'LMFAO': sum(user_laughter[user]['lmfao'] for user in users),\n",
    "        'HA Chains': sum(user_laughter[user]['ha_chains'] for user in users),\n",
    "        'ROFL': sum(user_laughter[user]['rofl'] for user in users)\n",
    "    }\n",
    "    \n",
    "    # Filter out zero values\n",
    "    total_by_type = {k: v for k, v in total_by_type.items() if v > 0}\n",
    "    \n",
    "    if total_by_type:\n",
    "        fig8 = px.pie(\n",
    "            values=list(total_by_type.values()),\n",
    "            names=list(total_by_type.keys()),\n",
    "            title='Distribution of Laughter Types'\n",
    "        )\n",
    "        fig8.update_layout(height=400)\n",
    "        fig8.show()\n",
    "\n",
    "else:\n",
    "    print(\"No laughter patterns found in the dataset.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAUGHTER ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
